# Performance Testing and Benchmarking Pipeline
# Comprehensive performance validation with multiple testing tools

name: Performance Testing

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    # Run performance tests weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test to run'
        required: false
        default: 'full'
        type: choice
        options:
        - full
        - load
        - stress
        - spike
        - volume
        - benchmark

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  PERFORMANCE_THRESHOLD_P95: '2000'  # 2 seconds
  PERFORMANCE_THRESHOLD_AVG: '1000'  # 1 second
  ERROR_THRESHOLD: '0.01'  # 1% error rate

jobs:
  # Setup Performance Testing Environment
  setup-environment:
    name: Setup Performance Testing Environment
    runs-on: ubuntu-latest
    outputs:
      test-url: ${{ steps.setup.outputs.test-url }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup test environment
        id: setup
        run: |
          echo "Setting up performance testing environment..."
          
          # Start application stack for testing
          docker-compose -f docker-compose.dev.yml up -d
          sleep 60
          
          # Wait for application readiness
          timeout 300 bash -c 'until curl -f http://localhost:8000/health; do sleep 5; done'
          
          echo "test-url=http://localhost:8000" >> $GITHUB_OUTPUT
          echo "✅ Performance testing environment ready"

      - name: Verify test environment
        run: |
          curl -f http://localhost:8000/health
          curl -f http://localhost:8000/api/v1/health

  # Load Testing with Locust
  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    needs: setup-environment
    if: github.event.inputs.test_type == 'load' || github.event.inputs.test_type == 'full' || github.event.inputs.test_type == ''
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install load testing tools
        run: |
          pip install locust requests

      - name: Create Locust test file
        run: |
          cat << 'EOF' > locustfile.py
          from locust import HttpUser, task, between
          import random
          
          class APIUser(HttpUser):
              wait_time = between(1, 3)
              
              def on_start(self):
                  """Initialize user session"""
                  pass
              
              @task(3)
              def health_check(self):
                  """Test health endpoint"""
                  self.client.get("/health")
              
              @task(2)
              def api_health_check(self):
                  """Test API health endpoint"""
                  self.client.get("/api/v1/health")
              
              @task(1)
              def api_endpoint(self):
                  """Test main API endpoints"""
                  # Add your API endpoints here
                  self.client.get("/api/v1/status")
          EOF

      - name: Run load test
        run: |
          locust -f locustfile.py \
            --host=${{ needs.setup-environment.outputs.test-url }} \
            --users=50 \
            --spawn-rate=5 \
            --run-time=5m \
            --headless \
            --html=load-test-report.html \
            --csv=load-test-results

      - name: Analyze load test results
        run: |
          python << 'EOF'
          import csv
          import json
          
          # Read Locust results
          results = {}
          try:
              with open('load-test-results_stats.csv', 'r') as f:
                  reader = csv.DictReader(f)
                  for row in reader:
                      if row['Name'] != 'Aggregated':
                          results[row['Name']] = {
                              'requests': int(row['Request Count']),
                              'failures': int(row['Failure Count']),
                              'avg_response_time': float(row['Average Response Time']),
                              'p95_response_time': float(row['95%ile']),
                              'requests_per_second': float(row['Requests/s'])
                          }
          except FileNotFoundError:
              print("Load test results file not found")
              results = {}
          
          # Performance validation
          performance_issues = []
          for endpoint, metrics in results.items():
              if metrics['avg_response_time'] > ${{ env.PERFORMANCE_THRESHOLD_AVG }}:
                  performance_issues.append(f"{endpoint}: Average response time {metrics['avg_response_time']:.0f}ms exceeds threshold")
              
              if metrics['p95_response_time'] > ${{ env.PERFORMANCE_THRESHOLD_P95 }}:
                  performance_issues.append(f"{endpoint}: P95 response time {metrics['p95_response_time']:.0f}ms exceeds threshold")
              
              error_rate = metrics['failures'] / max(metrics['requests'], 1)
              if error_rate > ${{ env.ERROR_THRESHOLD }}:
                  performance_issues.append(f"{endpoint}: Error rate {error_rate:.2%} exceeds threshold")
          
          # Save results
          with open('load-test-analysis.json', 'w') as f:
              json.dump({
                  'results': results,
                  'performance_issues': performance_issues,
                  'passed': len(performance_issues) == 0
              }, f, indent=2)
          
          if performance_issues:
              print("🚨 Performance issues detected:")
              for issue in performance_issues:
                  print(f"  - {issue}")
              exit(1)
          else:
              print("✅ Load test passed all performance thresholds")
          EOF

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: load-test-results
          path: |
            load-test-report.html
            load-test-results_*.csv
            load-test-analysis.json

  # Stress Testing with Artillery
  stress-testing:
    name: Stress Testing
    runs-on: ubuntu-latest
    needs: setup-environment
    if: github.event.inputs.test_type == 'stress' || github.event.inputs.test_type == 'full' || github.event.inputs.test_type == ''
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install stress testing tools
        run: |
          npm install -g artillery

      - name: Create Artillery stress test configuration
        run: |
          cat << 'EOF' > stress-test.yml
          config:
            target: ${{ needs.setup-environment.outputs.test-url }}
            phases:
              - duration: 60
                arrivalRate: 1
                name: "Warm up"
              - duration: 120
                arrivalRate: 5
                rampTo: 50
                name: "Ramp up load"
              - duration: 180
                arrivalRate: 50
                name: "Sustained load"
              - duration: 120
                arrivalRate: 50
                rampTo: 100
                name: "Spike load"
              - duration: 60
                arrivalRate: 1
                name: "Cool down"
            processor: "./stress-test-processor.js"
          scenarios:
            - name: "Health check stress test"
              weight: 40
              flow:
                - get:
                    url: "/health"
                    capture:
                      - json: "$.status"
                        as: "health_status"
            - name: "API stress test"
              weight: 60
              flow:
                - get:
                    url: "/api/v1/health"
                    capture:
                      - json: "$.status"
                        as: "api_status"
          EOF

      - name: Create Artillery processor
        run: |
          cat << 'EOF' > stress-test-processor.js
          module.exports = {
            logResponse: function(requestParams, response, context, ee, next) {
              console.log(`Response status: ${response.statusCode}, Response time: ${response.timings.response}ms`);
              return next();
            }
          };
          EOF

      - name: Run stress test
        run: |
          artillery run stress-test.yml --output stress-test-results.json

      - name: Generate stress test report
        run: |
          artillery report stress-test-results.json --output stress-test-report.html

      - name: Analyze stress test results
        run: |
          python << 'EOF'
          import json
          
          try:
              with open('stress-test-results.json', 'r') as f:
                  results = json.load(f)
              
              # Extract key metrics
              aggregate = results.get('aggregate', {})
              
              analysis = {
                  'total_requests': aggregate.get('requestsCompleted', 0),
                  'requests_per_second': aggregate.get('rps', {}).get('mean', 0),
                  'response_time_p50': aggregate.get('latency', {}).get('p50', 0),
                  'response_time_p95': aggregate.get('latency', {}).get('p95', 0),
                  'response_time_p99': aggregate.get('latency', {}).get('p99', 0),
                  'error_rate': aggregate.get('codes', {}).get('200', 0) / max(aggregate.get('requestsCompleted', 1), 1)
              }
              
              print(f"📊 Stress Test Results:")
              print(f"  Total Requests: {analysis['total_requests']}")
              print(f"  Requests/sec: {analysis['requests_per_second']:.2f}")
              print(f"  Response Time P50: {analysis['response_time_p50']:.0f}ms")
              print(f"  Response Time P95: {analysis['response_time_p95']:.0f}ms")
              print(f"  Response Time P99: {analysis['response_time_p99']:.0f}ms")
              
              # Save analysis
              with open('stress-test-analysis.json', 'w') as f:
                  json.dump(analysis, f, indent=2)
              
              # Performance validation
              if analysis['response_time_p95'] > ${{ env.PERFORMANCE_THRESHOLD_P95 }}:
                  print(f"🚨 Stress test failed: P95 response time {analysis['response_time_p95']:.0f}ms exceeds threshold")
                  exit(1)
              else:
                  print("✅ Stress test passed performance thresholds")
          
          except FileNotFoundError:
              print("❌ Stress test results file not found")
              exit(1)
          EOF

      - name: Upload stress test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: stress-test-results
          path: |
            stress-test-report.html
            stress-test-results.json
            stress-test-analysis.json

  # Database Performance Testing
  database-performance:
    name: Database Performance Testing
    runs-on: ubuntu-latest
    needs: setup-environment
    if: github.event.inputs.test_type == 'full' || github.event.inputs.test_type == ''
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install database performance tools
        run: |
          pip install psycopg2-binary sqlalchemy pytest-benchmark

      - name: Run database performance tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
        run: |
          python << 'EOF'
          import psycopg2
          import time
          import statistics
          import json
          
          # Database connection
          conn = psycopg2.connect(
              host="localhost",
              database="test_db",
              user="postgres",
              password="postgres"
          )
          
          def benchmark_query(query, iterations=100):
              times = []
              cursor = conn.cursor()
              
              for _ in range(iterations):
                  start_time = time.time()
                  cursor.execute(query)
                  cursor.fetchall()
                  end_time = time.time()
                  times.append((end_time - start_time) * 1000)  # Convert to ms
              
              cursor.close()
              return {
                  'avg_time_ms': statistics.mean(times),
                  'p95_time_ms': sorted(times)[int(0.95 * len(times))],
                  'min_time_ms': min(times),
                  'max_time_ms': max(times)
              }
          
          # Create test table
          cursor = conn.cursor()
          cursor.execute("""
              CREATE TABLE IF NOT EXISTS performance_test (
                  id SERIAL PRIMARY KEY,
                  name VARCHAR(100),
                  value INTEGER,
                  created_at TIMESTAMP DEFAULT NOW()
              )
          """)
          
          # Insert test data
          for i in range(1000):
              cursor.execute(
                  "INSERT INTO performance_test (name, value) VALUES (%s, %s)",
                  (f"test_{i}", i)
              )
          conn.commit()
          cursor.close()
          
          # Benchmark queries
          benchmarks = {
              'simple_select': benchmark_query("SELECT COUNT(*) FROM performance_test"),
              'indexed_select': benchmark_query("SELECT * FROM performance_test WHERE id = 500"),
              'range_select': benchmark_query("SELECT * FROM performance_test WHERE value BETWEEN 100 AND 200"),
              'aggregate_query': benchmark_query("SELECT AVG(value) FROM performance_test")
          }
          
          print("📊 Database Performance Results:")
          for query_name, results in benchmarks.items():
              print(f"  {query_name}:")
              print(f"    Average: {results['avg_time_ms']:.2f}ms")
              print(f"    P95: {results['p95_time_ms']:.2f}ms")
          
          # Save results
          with open('db-performance-results.json', 'w') as f:
              json.dump(benchmarks, f, indent=2)
          
          conn.close()
          print("✅ Database performance testing completed")
          EOF

      - name: Upload database performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: db-performance-results
          path: db-performance-results.json

  # Benchmark Testing
  benchmark-testing:
    name: Benchmark Testing
    runs-on: ubuntu-latest
    needs: setup-environment
    if: github.event.inputs.test_type == 'benchmark' || github.event.inputs.test_type == 'full' || github.event.inputs.test_type == ''
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install benchmark tools
        run: |
          pip install pytest-benchmark requests

      - name: Run Python benchmarks
        run: |
          pytest tests/performance/ --benchmark-only --benchmark-json=benchmark-results.json || true

      - name: Analyze benchmark results
        run: |
          python << 'EOF'
          import json
          import os
          
          if os.path.exists('benchmark-results.json'):
              with open('benchmark-results.json', 'r') as f:
                  results = json.load(f)
              
              print("📊 Benchmark Results:")
              for benchmark in results.get('benchmarks', []):
                  name = benchmark['name']
                  stats = benchmark['stats']
                  print(f"  {name}:")
                  print(f"    Mean: {stats['mean']:.6f}s")
                  print(f"    Stddev: {stats['stddev']:.6f}s")
                  print(f"    Min: {stats['min']:.6f}s")
                  print(f"    Max: {stats['max']:.6f}s")
              
              print("✅ Benchmark testing completed")
          else:
              print("No benchmark results found")
          EOF

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: benchmark-results.json

  # Cleanup Test Environment
  cleanup:
    name: Cleanup Test Environment
    runs-on: ubuntu-latest
    needs: [setup-environment, load-testing, stress-testing, database-performance, benchmark-testing]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Stop test environment
        run: |
          docker-compose -f docker-compose.dev.yml down -v
          echo "✅ Test environment cleaned up"

  # Performance Summary Report
  performance-summary:
    name: Performance Summary Report
    runs-on: ubuntu-latest
    needs: [load-testing, stress-testing, database-performance, benchmark-testing]
    if: always()
    steps:
      - name: Download all performance results
        uses: actions/download-artifact@v4

      - name: Generate performance summary
        run: |
          cat << 'EOF' > performance-summary.md
          # 🚀 Performance Testing Summary
          
          **Execution Date:** $(date)
          **Test Environment:** ${{ github.event_name }}
          **Performance Thresholds:**
          - Average Response Time: < ${{ env.PERFORMANCE_THRESHOLD_AVG }}ms
          - P95 Response Time: < ${{ env.PERFORMANCE_THRESHOLD_P95 }}ms
          - Error Rate: < ${{ env.ERROR_THRESHOLD }}
          
          ## 📊 Test Results
          
          ### Load Testing
          - **Status:** ${{ needs.load-testing.result }}
          - **Tool:** Locust
          - **Configuration:** 50 users, 5-minute duration
          
          ### Stress Testing
          - **Status:** ${{ needs.stress-testing.result }}
          - **Tool:** Artillery
          - **Configuration:** Ramp up to 100 concurrent users
          
          ### Database Performance
          - **Status:** ${{ needs.database-performance.result }}
          - **Tool:** Custom PostgreSQL benchmarks
          - **Queries:** SELECT, INSERT, UPDATE, DELETE operations
          
          ### Benchmark Testing
          - **Status:** ${{ needs.benchmark-testing.result }}
          - **Tool:** pytest-benchmark
          - **Scope:** Core application functions
          
          ## 🎯 Performance Metrics
          
          - **Response Time:** Measured across all endpoints
          - **Throughput:** Requests per second under load
          - **Error Rate:** Percentage of failed requests
          - **Resource Utilization:** CPU, memory, database connections
          
          ## 📈 Trends and Recommendations
          
          - Monitor performance trends over time
          - Optimize database queries if needed
          - Scale infrastructure based on load test results
          - Review error handling and timeout configurations
          
          ---
          
          *Generated by Performance Testing Pipeline - ${{ github.run_id }}*
          EOF

      - name: Upload performance summary
        uses: actions/upload-artifact@v4
        with:
          name: performance-summary
          path: performance-summary.md

      - name: Create performance issue for failures
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Performance Test Failures - ${new Date().toISOString().split('T')[0]}`,
              body: `Performance testing has detected issues that require attention:
              
              **Failed Tests:**
              - Load Testing: ${{ needs.load-testing.result }}
              - Stress Testing: ${{ needs.stress-testing.result }}
              - Database Performance: ${{ needs.database-performance.result }}
              - Benchmark Testing: ${{ needs.benchmark-testing.result }}
              
              Please review the performance test results and optimize as needed.`,
              labels: ['performance', 'testing', 'automated', 'needs-investigation']
            })